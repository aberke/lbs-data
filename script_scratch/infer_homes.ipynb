{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/mount/201805/14460_201805.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mount the remote disk to work with data.\n",
    "\n",
    "\n",
    "sshfs -o allow_other,defer_permissions \\\n",
    "USER@matlaber1.media.mit.edu:/REMOTE_DATA_PATH \\\n",
    "/LOCAL_DATA_PATH/\n",
    "\n",
    "later...\n",
    "umount /LOCAL_DATA_PATH/\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# How is this data different than that in other files, for different MSA's or months?\n",
    "MSA = \"14460\"\n",
    "TIME_PERIOD = \"201805\"\n",
    "\n",
    "data_path = \"../data/mount/{time_period}/\".format(time_period=TIME_PERIOD)\n",
    "filename = \"{msa}_{time_period}.csv\".format(msa=MSA, time_period=TIME_PERIOD)\n",
    "filepath = data_path + filename\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables that will be used at both entry and re-entry parts of script\n",
    "\n",
    "DEVICE_ID = 'device ID'\n",
    "DEVICE_ID_OCCURANCES = 'device ID occurances'\n",
    "DAY = 'day'\n",
    "DEVICE_ID_DAYS_COUNT = 'days count'\n",
    "\n",
    "DWELLTIME = 'DwellTime'\n",
    "TIMESTAMP = 'timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility for testing w. lower bandwidth: read in just a random sample of data.\n",
    "\n",
    "NOTE: can only do this for scratch/testing code, etc.  The data will lose a lot\n",
    "because this is not a random sample of USERs - it's a random sample of data points\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "DEFAULT_RANDOM_SAMPLE_PORTION = 0.01 # to keep random % of the lines\n",
    "\n",
    "def read_csv_random_sample(filepath, portion=DEFAULT_RANDOM_SAMPLE_PORTION):\n",
    "    return pd.read_csv(\n",
    "         filepath,\n",
    "         header=0, \n",
    "         skiprows=lambda i: i>0 and random.random() > portion\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86695, 7)\n"
     ]
    }
   ],
   "source": [
    "# FOR TESTING / DEBUGGING\n",
    "# reading in small random sample of file\n",
    "df = read_csv_random_sample(filepath)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8680859, 7)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data of interest. (All of it)\n",
    "df = pd.read_csv(filepath)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the columns to use 'device ID instead of LBS provider name'\n",
    "df.rename(columns={df.columns[2]:DEVICE_ID}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 166686 of 8680859 rows where dwelltime > maximum of 1440\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Filter out garbage / outlier datapoints.\"\"\"\n",
    "\n",
    "# Filter out rows where dwelltime > MAX_DWELLTIME\n",
    "MAX_DWELLTIME = 24*60 # 24 hours\n",
    "row_count_before = df.shape[0]\n",
    "df = df[df[DWELLTIME] <= MAX_DWELLTIME]\n",
    "rows_dropped = (row_count_before - df.shape[0])\n",
    "print('dropped %s of %s rows where dwelltime > maximum of %s' % (rows_dropped, row_count_before, MAX_DWELLTIME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique device ids: 250857\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Filter the data to users who have enough data points.\n",
    "Filter data to users with:\n",
    "- data points for (>15 days) at least half the days of the month\n",
    "- enough data for each day: avg of 2 datapoints for each day they are represented\n",
    "\n",
    "(0) Preliminary step:\n",
    "Filter to users with at least USER_TOTAL_POINTS_THRESHOLD (32) datapoints\n",
    "- find # of occurances for each user ID\n",
    "- filter out users with < THRESHOLD # occurances\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "USER_TOTAL_DAYS_THRESHOLD = 16\n",
    "USER_TOTAL_OCCURANCES_THRESHOLD = 32 # require at least avg of 2 points per day (where # days at least 16)\n",
    "\n",
    "\n",
    "unique_device_IDs = df[DEVICE_ID].nunique()\n",
    "print('unique device ids:', unique_device_IDs)\n",
    "df.sort_values(by=[DEVICE_ID], inplace=True)\n",
    "# df.head() # check: yeah that looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count before:  8514173\n",
      "user count before:  250857\n",
      "rows dropped:  2603129 ; remaining rows: 5911044\n",
      "users dropped:  200118 ; remaining users: 50739\n"
     ]
    }
   ],
   "source": [
    "# Add value counts column to DF.\n",
    "# i.e. add counts for number of occurances of given device ID\n",
    "df[DEVICE_ID_OCCURANCES] = df[DEVICE_ID].map(df[DEVICE_ID].value_counts())\n",
    "\n",
    "# drop rows where value count for that row's device ID < USER_TOTAL_POINTS_THRESHOLD\n",
    "row_count_before = df.shape[0]\n",
    "user_count_before = df[DEVICE_ID].nunique()\n",
    "print('row count before: ', row_count_before)\n",
    "print('user count before: ', user_count_before)\n",
    "df = df[df[DEVICE_ID_OCCURANCES] >= USER_TOTAL_OCCURANCES_THRESHOLD]\n",
    "df.sort_values(by=[DEVICE_ID_OCCURANCES, DEVICE_ID], ascending=True, inplace=True)\n",
    "\n",
    "\n",
    "# Add count of how many unique days on which this user is represented in the dataset\n",
    "# map device ID to # unique days\n",
    "DEVICE_ID_to_days_df = df.groupby(DEVICE_ID)[DAY].nunique()\n",
    "df[DEVICE_ID_DAYS_COUNT] = df[DEVICE_ID].map(DEVICE_ID_to_days_df)\n",
    "# before dropping, check:\n",
    "# check: Does this look right?  sort the data by ascending number of days represented, and take a look\n",
    "df.sort_values(by=[DEVICE_ID_DAYS_COUNT, DEVICE_ID], inplace=True)\n",
    "# drop rows for users with less than USER_TOTAL_DAYS_THRESHOLD\n",
    "df = df[df[DEVICE_ID_DAYS_COUNT] >= USER_TOTAL_DAYS_THRESHOLD]\n",
    "# how much did we drop?\n",
    "rows_dropped = row_count_before - df.shape[0]\n",
    "users_dropped = user_count_before - df[DEVICE_ID].nunique()\n",
    "print('rows dropped: ', rows_dropped, '; remaining rows:', df.shape[0])\n",
    "print('users dropped: ', users_dropped, '; remaining users:', len(df[DEVICE_ID].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this filtered dataset\n",
    "filtered_filepath = data_path + \"filtered/\" + \"filtered_\" + filename\n",
    "df.to_csv(filtered_filepath)\n",
    "# This is a stopping point - you can come back and read the data in from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Entry Point for already filtered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_filepath = data_path + \"filtered/\" + \"filtered_\" + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING / DEBUGGING\n",
    "# Read small random sample of previously filter data\n",
    "df = read_csv_random_sample(filtered_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all previously filtered data\n",
    "df = pd.read_csv(filtered_filepath)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit data to random sample of users (users not datapoints)\n",
    "\n",
    "sample_portion = 0.3\n",
    "\n",
    "user_ids = df[DEVICE_ID].unique()\n",
    "sample_count = int(len(user_ids)*sample_portion)\n",
    "sampled_user_ids = random.sample(list(user_ids), sample_count)\n",
    "print(\"keeping only {sample}% sample of users datapoints\".format(sample=sample_portion*100))\n",
    "df = df[df[DEVICE_ID].isin(sampled_user_ids)]\n",
    "print(\"%s remaining users\" % sample_count)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Home Location (as census area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer home census area (tract/block group)s:\n",
    "\n",
    "make a DF where its the same data, but limited to nighttime points\n",
    "- Define a time range for night time (in hours)\n",
    "- Add a column for 'nighttime dwelltime' as the dwelltime that occurs within those nighttime hours\n",
    "    - i.e. if datapoint has a dwelltime of 3 hours starting at 7pm, while starting nighttime hour is 8pm then nighttime dwelltime for this datapoint is 2 hours\n",
    "\n",
    "make nighttime DF as the subset of data where the nighttime dwelltime > 0\n",
    "\n",
    "filter out users for whome there are less than required threshold of nighttime datapoints\n",
    "\n",
    "assign census area (tract/block group) to each point in nighttime df\n",
    "- lat,lon point falls within area --> row assigned to area\n",
    "- this is added as a column\n",
    "\n",
    "infer home geoid as area where user has largest nighttime dwelltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on Working with timestamps.\n",
    "The timestamps are in ISO format.  e.g. '2018-05-01T07:18:02-04:00'\n",
    "The offset refers to how many hours the timezone is from Coordinated Universal Time (UTC).\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz, parser\n",
    "\n",
    "\n",
    "# columns I add:\n",
    "# local time for when dwell starts and ends\n",
    "DWELLTIME_START = 'dwelltime start'\n",
    "NIGHTTIME_DWELLTIME = 'nighttime dwelltime'\n",
    "\n",
    "\n",
    "NIGHTTIME_START_HOUR = 20 # 8pm\n",
    "NIGHTTIME_END_HOUR = 8\n",
    "\n",
    "\n",
    "def timedelta_minutes(tdelta):\n",
    "    \"\"\"Helper function. Returns minutes for timedelta object.\"\"\"\n",
    "    return round((tdelta.seconds / 60), 2)\n",
    "\n",
    "# tests for the timedelta_minutes function\n",
    "assert(timedelta_minutes(timedelta(minutes=230.95)) == 230.95)\n",
    "assert(timedelta_minutes(timedelta(minutes=3.37)) == 3.37)\n",
    "\n",
    "\n",
    "# add local start for dwelltime\n",
    "def dwelltime_start(tstamp):\n",
    "    # tstamp may be a datettime.time or str -- handle either\n",
    "    return parser.parse(str(tstamp))\n",
    "\n",
    "df[DWELLTIME_START] = df[TIMESTAMP].apply(dwelltime_start)\n",
    "\n",
    "\n",
    "# calculate nighttime dwelltime\n",
    "def nighttime_dwelltime(row):\n",
    "    \"\"\"Returns the portion of the dwelltime that occurred in the nighttime hours.\"\"\"\n",
    "    dwelltime = timedelta(minutes=float(row[DWELLTIME]))\n",
    "    dwell_start = row[DWELLTIME_START]\n",
    "    dwell_start_hour = dwell_start.time().hour\n",
    "    dwell_end = dwell_start + dwelltime\n",
    "    dwell_end_hour = dwell_end.time().hour\n",
    "    # Return zero for dwelltimes that do not overlap with the nighttime hours.\n",
    "    if (dwell_start_hour > NIGHTTIME_END_HOUR) \\\n",
    "        and (dwell_start_hour < NIGHTTIME_START_HOUR) \\\n",
    "        and (dwell_end_hour < NIGHTTIME_START_HOUR) \\\n",
    "        and (dwell_end_hour > NIGHTTIME_END_HOUR):\n",
    "        return 0\n",
    "    \n",
    "    if (dwell_start.time().hour <= NIGHTTIME_END_HOUR):\n",
    "        # dwelltime starts in the early hours of the morning, before nighttime ends.\n",
    "        # restrict the dwelltime to the nighttime hours\n",
    "        nighttime_end_dt = datetime(dwell_start.year,  dwell_start.month, dwell_start.day, hour=NIGHTTIME_END_HOUR, tzinfo=dwell_start.tzinfo)\n",
    "        nighttime_dwelltime = min((nighttime_end_dt - dwell_start), dwelltime)\n",
    "    \n",
    "    else: # otherwise... the dwelltime starts before midnight\n",
    "        nighttime_dwelltime = dwelltime\n",
    "        # if it start before the nighttime starts, subtract the portion before nighttime starts\n",
    "        nighttime_start_dt = datetime(dwell_start.year,  dwell_start.month, dwell_start.day, hour=NIGHTTIME_START_HOUR, tzinfo=dwell_start.tzinfo)\n",
    "        if dwell_start < nighttime_start_dt:\n",
    "            delta = nighttime_start_dt - dwell_start\n",
    "            nighttime_dwelltime -= delta\n",
    "        \n",
    "        # if the dwell continues until the next day, and\n",
    "        # if the dwell ends after the nighttime ends,\n",
    "        # then subtract the portion that happens after nighttime ends\n",
    "        if ((dwell_end_hour < NIGHTTIME_START_HOUR) and (dwell_end_hour > NIGHTTIME_END_HOUR)):\n",
    "            # the nighttime end for this case is the next day\n",
    "            nighttime_end_dt = datetime(dwell_end.year,  dwell_end.month, dwell_end.day, hour=NIGHTTIME_END_HOUR, tzinfo=dwell_end.tzinfo)\n",
    "            delta = dwell_end - nighttime_end_dt\n",
    "            nighttime_dwelltime -= delta\n",
    "    return timedelta_minutes(nighttime_dwelltime)\n",
    "        \n",
    "    \n",
    "# add nighttime dwelltime\n",
    "df[NIGHTTIME_DWELLTIME] = df[[DEVICE_ID, TIMESTAMP, DWELLTIME, DWELLTIME_START]].apply(nighttime_dwelltime, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make additional DF limited to nighttime datapoints \n",
    "print('df all rows count:', df.shape[0])\n",
    "print('df unique users count:', df[DEVICE_ID].nunique())\n",
    "nighttime_df = df[df[NIGHTTIME_DWELLTIME] > 0].copy()\n",
    "print('nighttime df rows count:', nighttime_df.shape[0])\n",
    "print('nighttime df unique users count:', nighttime_df[DEVICE_ID].nunique())\n",
    "# nighttime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the users for whom there are fewer than threshold of nighttime data points\n",
    "# (do this filtering before the computationally expensive process of putting points in census areas)\n",
    "\n",
    "# What is the right threshold?\n",
    "USER_NIGHTTIME_POINTS_THRESHOLD = 4\n",
    "\n",
    "user_ids = nighttime_df[DEVICE_ID].unique()\n",
    "\n",
    "user_nights = nighttime_df.groupby([DEVICE_ID, DAY])[DEVICE_ID].count().groupby(level=DEVICE_ID).count()\n",
    "keep_user_ids = user_nights[user_nights >= USER_NIGHTTIME_POINTS_THRESHOLD].index.tolist()\n",
    "\n",
    "print('filtered out users with fewer than %s unique nights of data' % USER_NIGHTTIME_POINTS_THRESHOLD)\n",
    "print('user count before:', len(user_ids), '\\nuser count after filtering:', len(keep_user_ids))\n",
    "nighttime_df = nighttime_df[nighttime_df[DEVICE_ID].isin(keep_user_ids)]\n",
    "print('remaining nighttime datapoints: ', nighttime_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign census tract/block to each point in nighttime df\n",
    "# - lat,lon point falls within area --> row assigned to area\n",
    "# - this is added as a column\n",
    "#\n",
    "# BUT FIRST: filter out points that do not fall within target counties\n",
    "# map geolocation to countysub & county within shapefile\n",
    "# The shapefile is for a limited set of counties that contain target census tracts\n",
    "# reason this is good filtering:\n",
    "# - fewer county shapes than tracts for given area, so less computationally intensive\n",
    "# - then filter out data that did not fall within target counties (i.e. in other parts of MSA)\n",
    "\n",
    "from datetime import datetime\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "COUNTY_FP = \"COUNTYFP\"\n",
    "COUNTY_SUBDIVISION_FP = \"COUSUBFP\"\n",
    "print('NOTE: using limited area of boston, brookline, cambridge, somerville, so most datapoints will not fall in this area')\n",
    "county_subdivs_shapefile_filepath = \"./shapefiles/ma/boston-brookline-cambridge-somerville_countysubdivisions.shp\"\n",
    "county_subdivs_shapefile = geopandas.read_file(county_subdivs_shapefile_filepath)\n",
    "county_subdivs_shapefile = county_subdivs_shapefile.to_crs(epsg=4326)\n",
    "county_subdivs_shapefile.plot()\n",
    "\n",
    "def get_county_subdiv(row):\n",
    "    r = random.randint(1, 10000)\n",
    "    if r == 1:\n",
    "        print(datetime.now(), \"get county subdiv\")\n",
    "    lat = float(row['lat'])\n",
    "    lon = float(row['lon'])\n",
    "    point = Point(lon, lat) # the points actually reverse - yes this looks weird but consider lon the x-axis and lat the y-axis on an x-y plane...\n",
    "    for i, row in county_subdivs_shapefile.iterrows():\n",
    "        try:\n",
    "            # this could be a Polygon or a Multipolygon when the tract contains islands (<3 New England)\n",
    "            polygon = row['geometry']\n",
    "            if point.within(polygon):\n",
    "                return row[COUNTY_SUBDIVISION_FP]\n",
    "        except Exception as e:\n",
    "            print('Exception when looking for point in county subdivision ' + row[COUNTY_SUBDIVISION_FP], ': ', e)\n",
    "            raise e\n",
    "    return None\n",
    "\n",
    "nighttime_df[COUNTY_SUBDIVISION_FP] = nighttime_df.apply(get_county_subdiv, axis=1)\n",
    "nighttime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nighttime_df = nighttime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows for datapoints where census area could not be determined\n",
    "rows_before = filtered_nighttime_df.shape[0]\n",
    "users_count_before = filtered_nighttime_df[DEVICE_ID].nunique()\n",
    "filtered_nighttime_df.dropna(inplace=True)\n",
    "rows_after = filtered_nighttime_df.shape[0]\n",
    "users_count_after = filtered_nighttime_df[DEVICE_ID].nunique()\n",
    "print(\"dropped %s rows where county subdivision not determined. rows remaining: %s\" % (rows_before - rows_after, rows_after))\n",
    "print(\"users before: %s \\nusers after: %s\" % (users_count_before, users_count_after))\n",
    "\n",
    "# filtered_nighttime_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Filtered Data Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this filtered dataset\n",
    "\n",
    "filtered_nighttime_filepath = data_path + \"filtered/\" + \"filtered_\" + \"{msa}_{time_period}_nighttime_bos_brook_cam_som.csv\".format(msa=MSA, time_period=TIME_PERIOD)\n",
    "print(\"saving data to file\", filtered_nighttime_filepath)\n",
    "filtered_nighttime_df.to_csv(filtered_nighttime_filepath)\n",
    "# This is a stopping point - you can come back and read the data in from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attach census block group and census tract information to each datapoint.\n",
    "A census tract contains many block groups.\n",
    "The GEOID for a block group is 12 digits, where the GEOID for a census tract is 11 digits.\n",
    "The first 11 digits in a block group are the census tract.\n",
    "Thus a single function can determine both the block group and census tract.\n",
    "\n",
    "about GEOIDs: https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html\n",
    "\"\"\"\n",
    "\n",
    "BLOCKGROUP_GEOID = 'BLOCKGROUP GEOID'\n",
    "\n",
    "print('NOTE: using limited area of boston, brookline, cambridge, somerville, so most datapoints will not fall in this area')\n",
    "census_blockgroups_shapefile_filepath = \"./shapefiles/ma/boston-brookline-cambridge-somerville_blockgroup.shp\"\n",
    "# census_tracts_shapefile_filepath = \"./shapefiles/ma/msa_11460_census2010_blockgroups.shp\"\n",
    "blockgroups = geopandas.read_file(census_tracts_shapefile_filepath)\n",
    "blockgroups = blockgroups.to_crs(epsg=4326)\n",
    "blockgroups.plot()\n",
    "\n",
    "\n",
    "def get_census_blockgroup(row):\n",
    "    if random.randint(1, 1000) == 1:\n",
    "        print(datetime.now(), \"get census blockgroup\")\n",
    "    lat = float(row['lat'])\n",
    "    lon = float(row['lon'])\n",
    "    point = Point(lon, lat) # the points actually reverse - yes this looks weird but consider lon the x-axis and lat the y-axis on an x-y plane...\n",
    "    for i, blockgroups_row in blockgroups.iterrows():\n",
    "        try:\n",
    "            # this could be a Polygon or a Multipolygon when the tract contains islands (<3 New England)\n",
    "            blockgroups_polygon = blockgroups_row['geometry']\n",
    "            if point.within(blockgroups_polygon):\n",
    "                return blockgroups_row['GEOID10']\n",
    "        except Exception as e:\n",
    "            print('Exception when looking for point in census block group ' + blockgroups_row['GEOID10'], ': ', e)\n",
    "            raise e\n",
    "    return None\n",
    "\n",
    "\n",
    "filtered_nighttime_df[BLOCKGROUP_GEOID] = nighttime_df.apply(get_census_blockgroup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the tract GEOID from the blockgroup GEOID\n",
    "TRACT_GEOID = 'TRACT GEOID'\n",
    "\n",
    "def blockgroup_geoid_to_tract_geoid(blockgroup_geoid):\n",
    "    blockgroup_geoid = str(blockgroup_geoid)\n",
    "    return blockgroup_geoid[:-1]\n",
    "\n",
    "filtered_nighttime_df[TRACT_GEOID] = filtered_nighttime_df[BLOCKGROUP_GEOID].apply(blockgroup_geoid_to_tract_geoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows for datapoints where census area could not be determined\n",
    "rows_before = filtered_nighttime_df.shape[0]\n",
    "users_count_before = filtered_nighttime_df[DEVICE_ID].nunique()\n",
    "filtered_nighttime_df.dropna(inplace=True)\n",
    "rows_after = filtered_nighttime_df.shape[0]\n",
    "users_count_after = filtered_nighttime_df[DEVICE_ID].nunique()\n",
    "print(\"dropped %s rows where census area not determined. rows remaining: %s\" % (rows_before - rows_after, rows_after))\n",
    "print(\"users before: %s \\nusers after: %s\" % (users_count_before, users_count_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nighttime_filepath = data_path + \"filtered/\" + \"filtered_\" + \"{msa}_{time_period}_nighttime_blockgroup_bos_brook_cam_som.csv\".format(msa=MSA, time_period=TIME_PERIOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving data to file\", filtered_nighttime_filepath)\n",
    "filtered_nighttime_df.to_csv(filtered_nighttime_filepath)\n",
    "# This is a stopping point - you can come back and read the data in from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up the data if needed\n",
    "# Read all previously filter data\n",
    "print(\"reading data from file\", filtered_nighttime_filepath)\n",
    "filtered_nighttime_df = pd.read_csv(filtered_nighttime_filepath)\n",
    "print(filtered_nighttime_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"infer homes: (V1)\n",
    "infer most likely home census area for user\n",
    "choose geoid where the user dwells for the most time in the nighttime hours\n",
    "\n",
    "\n",
    "- make map for each user as {geoid: cumulative nighttime dwelltime, for each geoid they are present at in nighttime hours}\n",
    "- (where geoids are ids for census areas)\n",
    "- then choose the geoid with the greatest cumulative nighttime dwelltime\n",
    "\"\"\"\n",
    "\n",
    "inferred_homes_map = {}\n",
    "user_ids = filtered_nighttime_df[DEVICE_ID].unique()\n",
    "print('inferring home areas for %s users' % len(user_ids))\n",
    "for i, user_id in np.ndenumerate(user_ids):\n",
    "    if (i[0] % 1000) == 0:\n",
    "        print(i[0], datetime.now())\n",
    "    # Make map for user as {geoid: cumulative dwelltime, for each geoid they are present at in nighttime hours}\n",
    "    user_map = {}\n",
    "    # Get the mapping of geoids to dwelltimes for given user\n",
    "    u_df = filtered_nighttime_df[filtered_nighttime_df[DEVICE_ID] == user_id][[BLOCKGROUP_GEOID, NIGHTTIME_DWELLTIME]]\n",
    "    for r, row in u_df.iterrows():\n",
    "        geoid = row[BLOCKGROUP_GEOID]\n",
    "        nighttime_dwelltime = row[NIGHTTIME_DWELLTIME]\n",
    "        if geoid not in user_map:\n",
    "            user_map[geoid] = 0\n",
    "        user_map[geoid] += nighttime_dwelltime\n",
    "    \n",
    "    # choose the geoid with the greatest dwelltime\n",
    "    max_dwelltime = 0\n",
    "    max_dwelltime_geoid = None\n",
    "    for geoid, nighttime_dwelltime in user_map.items():\n",
    "        if nighttime_dwelltime > max_dwelltime:\n",
    "            max_dwelltime = nighttime_dwelltime\n",
    "            max_dwelltime_geoid = geoid\n",
    "        \n",
    "    inferred_homes_map[user_id] = max_dwelltime_geoid\n",
    "\n",
    "print(\"made inferred homes map for %s users\" % len(inferred_homes_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add inferred home area as colum in the larger (nighttime) DF.\"\"\"\n",
    "\n",
    "INFERRED_HOME_TRACT_GEOID = \"inferred home census tract geoid\"\n",
    "INFERRED_HOME_BLOCKGROUP_GEOID = \"inferred home census blockgroup geoid\"\n",
    "\n",
    "filtered_nighttime_df[INFERRED_HOME_BLOCKGROUP_GEOID] = filtered_nighttime_df[DEVICE_ID].map(inferred_homes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this dataset\n",
    "filtered_nighttime_filepath = data_path + \"filtered/\" + \"filtered_\" + \"{msa}_{time_period}_inferred_homes_blockgroup_bos_brook_cam_som.csv\".format(msa=MSA, time_period=TIME_PERIOD)\n",
    "print(\"saving data to file\", filtered_nighttime_filepath)\n",
    "filtered_nighttime_df.to_csv(filtered_nighttime_filepath)\n",
    "# This is a stopping point - you can come back and read the data in from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the users to home census area.\n",
    "# i.e. map inferred home census area geoid to number of users\n",
    "# make DF with columns: INFERRED_HOME_CENSUS_TRACT_GEOID, USER_COUNT\n",
    "\n",
    "USER_COUNT = \"deviceID count\"\n",
    "\n",
    "inferred_home_geoids = nighttime_df[INFERRED_HOME_TRACT_GEOID].unique()\n",
    "inferred_home_geoids_user_count = []\n",
    "for geoid in inferred_home_geoids:\n",
    "    geoid_df = nighttime_df[nighttime_df[INFERRED_HOME_TRACT_GEOID] == geoid]\n",
    "    user_count = geoid_df[DEVICE_ID].nunique()\n",
    "    inferred_home_geoids_user_count.append(user_count)\n",
    "\n",
    "inferred_homes_df = pd.DataFrame.from_dict({\n",
    "    INFERRED_HOME_TRACT_GEOID: inferred_home_geoids,\n",
    "    USER_COUNT: inferred_home_geoids_user_count,\n",
    "})\n",
    "print(inferred_homes_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
