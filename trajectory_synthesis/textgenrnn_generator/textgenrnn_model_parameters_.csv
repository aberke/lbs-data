,default,default,default,A,A,A,B,B,B,C,C,C,D,D,D,E,E,E,O,O,O,default-bidirectional,default-bidirectional,default-bidirectional,A-bidirectional,A-bidirectional,A-bidirectional,B-bidirectional,B-bidirectional,B-bidirectional,C-bidirectional,C-bidirectional,C-bidirectional,D-bidirectional,D-bidirectional,D-bidirectional,E-bidirectional,E-bidirectional,E-bidirectional,O-bidirectional,O-bidirectional,O-bidirectional,N,N,N,original,original,original,P,P,P,P-bidirectional,P-bidirectional,P-bidirectional,P-bidirectional,Q-bidirectional,Q-bidirectional,Q-bidirectional,Q-bidirectional,R-bidirectional,R-bidirectional,R-bidirectional,S-bidirectional,S-bidirectional,S-bidirectional,T-bidirectional,T-bidirectional,T-bidirectional,U-bidirectional,U-bidirectional,U-bidirectional,V-bidirectional,V-bidirectional,V-bidirectional,V-bidirectional,W-bidirectional,W-bidirectional,W-bidirectional,W-bidirectional,X-bidirectional,X-bidirectional,X-bidirectional,X-bidirectional,X-bidirectional,Y-bidirectional,Y-bidirectional,Y-bidirectional,Y-bidirectional,Y-bidirectional,Z-bidirectional,Z-bidirectional,Z-bidirectional,Z-bidirectional,Z-bidirectional
rnn_bidirectional,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE
max_length,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,48,48,48,48,48,48,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,48,48,48,48,48,48,50,50,50,50,50,50,50,50,50,50,50,50,50,60,60,60,60,50,50,50,50,50,50,50,50,50,60,60,60,70,70,70,70,84,84,84,84,60,60,60,60,60,60,60,60,60,60,72,72,72,72,72
rnn_layers,2,2,2,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,2,2,2,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2
rnn_size,128,128,128,128,128,128,256,256,256,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,256,256,256,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,256,256,256,256,256,256,128,128,128,128,128,128,128,128,128,128,128,128,128,256,256,256,256,256,256,256,256,256,256
dropout,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.3,0.3,0.3,0.4,0.4,0.4,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3
dim_embeddings,50,50,50,50,50,50,50,50,50,50,50,50,100,100,100,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,100,100,100,50,50,50,50,50,50,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,64,64,64,128,128,128,100,100,100,100,100,100,128,128,128,128,128,128,128,128,128,128,128,128,128,100,100,100,100,100,100,100,100,100,100
temperature,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.8,0.8,0.8,0.9,1,0.8,0.9,1,1.1,0.8,0.9,1,1.1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,0.8,0.9,1,1.1,0.8,0.9,1,1.1,0.8,0.9,1,1.1,1.2,0.8,0.9,1,1.1,1.2,0.8,0.9,1,1.1,1.2
batch size,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,modifies original by not being bidirectional,,,,,,training  in progress on matlaber4,,PID 14858,modifies original with dropout,,,,modifies original with increased max_length,,,,modifies original with smaller embedding,,,modifies original with RNN layers,,,finished training with final loss of 0.668,,,it died twice!!,,,,,,,,,,,combines successful Q-bi and P-bi parameter changes,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,model trained...,model trained...,model trained...,model trained...,model trained...,model trained...,,,,,,,,,,,,,,,,,,,,,,final training loss,0.63,,name:,synthetic_trajectories_,,,,,done training,,,,training  on matlaber11,PID,1812,1812,,,,,,,,,,training again on matlaber11,PID,13715,training on matlaber11,killed?!,,,killed after 25 epochs,,,,killed after 29 epochs,,,,,killed after 6 epochs,loss: 0.71,,,,,,,,
generated for cambridge,GPU,yes,yes,yes,yes,yes,no,no,no,yes,yes,yes,yes,yes,yes,no,no,no,no,no,no,remote,done - need to copy over,done - need to copy over,yes,yes,yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,loss after epoch 19:,0.65,,,
generated for general sample,GPU,yes,yes,TODO,TODO,,yes,yes,yes,done,done,done,TODO,TODO,TODO,yes,yes,yes,yes,yes,yes,yes,yes,yes,yes,yes,yes,,,,,,,,,,,,,,,,,,,done,done,done,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,loss  after 28 epochs:,0.6,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,UH OH! something weird happend here.  loss goes to nan...,,,,,,,,,,,,looks like loss plateaus around epoch  12ish,,,loss after 26 epochs: 0.59ish,,,,loss at 0.54ish after 30 epochs,,,,generated  prematurely after 15 epochs where loss ~0.65,,,generated after 37 epochs,,,done: generated,,,,,,after ~17 epochs: ~0.64,,,,loss after 25 epochs:,0.847,,,after ~15 epochs: 0.65,,,,,training again on matlaber11,PID,14388,,,loss after 41 epochs:,0.53,generated after 41 epochs,,
,,,,,,,,,,, ,output looked normal for loss,,,,,,,training wierdness: UH OH!,,,,,,loss looks normal,,,,,,,,,,,,again!,,,epoch 16: loss 0.61ish,,,TODO? generate?,,,,,,generated at epoch 16,,,(done) generated after 26 epochs on matlaber4,,,,(done) generated prematurely  after 30 epochs,,,,(done) generated prematurely again at 35 epochs,,,not worth  continuing....,,,,,,epoch 9: loss 0.7,,,after 33 epochs:,~0.56,,,weird because had earlier losses lower ~0.65,,,,after 29 epochs:,0.57,,,,,,still  in progress,,,,,,,
,,,,,,,,,,,,finished after 72 epochs,,,,,,,loss at 18/75 epochs: 0.714,,,,,,finished after 67 epochs,,,,,,,,,,,,new training,at  epoch 20ish loss goes to nan  again!,,generated after  training on 17 epochs,,,,,,,,,,,,saw home label match rate of 0.809,,saw home label match rate of 0.8315,saw home label match rate of 0.8315,,saw home label match rate of  0.845,saw home label match rate of 0.848,saw home label match rate of 0.848,weird: loss started fluctuating and going up again,,,,,,,,,epoch 19: loss 0.68,,,(done) generated after 33 epochs,matlaber1,,,done generating after 25 epochs on matlaber7,,,,(done) generated after 29 epochs,,had home match rate of ,,,,,loss after 13 epochs:  0.67,,,loss after 49 epochs:,0.5,,,
,,,,,,,,,,,,,,,,,,,then goes up!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,but it didn't plateau! loss continued to decreate,,,BUT loss still giong down!,epoch 42: 0.51,,,BUT loss still  going down!,0.42 after 45 epochs,,,,,,DONE,,,,,,epoch 27: loss 0.67,,,,,,,,,,,after 50 epochs (retrained) loss:,0.63,,,,,,loss  after  18 epochs: 0.7,,,finished after 50 epochs at ,0.5 loss,generated after 50 epochs,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(done) generated again at 42 epochs,,,done after 50 epochs:,loss: 0.49,,,(done) generated again after 45 epochs on matlaber1,,,,,,,,,,,,,,,,,,,,,,,,(done) generated after 50 epochs,,,,,,,loss after 25 epochs: 0.7,generated after 25 epochs,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,time to generate!  this loss  isn't going down...,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,had home atch rates of: 0.84,0.872,0.86,0.86,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,values went down after regenerating! over  training happens!,,,,,,,,,,,,,,